---
title: 如何使用Python 來做web scraping?
date: 2016-09-08 16:11:26
tags: [data science, cs109, scraping]
categories: [data science, cs109]
---
## 第二講
這是harvard cs 109第二講的內容整理。其實還有pandas的介紹。這留在之後再繼續寫。今天要來整理的是web scraping。
之前讀了不少如何當網路爬蟲的文章，不過我覺得自己上手做一次更快、更好，學得更多。建議想要學習scraping的各位，可以依影片，自己練習一次就會很有感覺了。在玩數據之前，要先有數據啊，本課有提到如果是有api, 或是有csv等整理好的檔案，可以用Pandas的read_csv，直接建好DataFrame；但是如果是網頁的話，就要自行先把網頁爬下來，再給pandas處理。所以，開始成為data scientist之前，要先會web scraping !

因為影片的code都是python 2.7為主，考量到2020就[不再支援](https://pythonclock.org/)，我自己是直接使用python 3.5來學習。不過還是有遇到一些2to3的問題。如果不想要花時間解決2to3的問題（但遲早還是要面對啊），可以直接使用2.7。
![2020](https://dl.dropboxusercontent.com/u/22163115/Selection_016.png)
- 第二講的web scraping 的[ipython notebook](https://github.com/cs109/2015/blob/master/Lectures/02-DataScraping.ipynb)
- [影片](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f7ff1893-fbf7-4909-b44e-12e61a98a677)

<!--more-->

## web scraping with python
一個小時的課程有介紹Pandas和ipython notebook的使用和快速web scarping 上手。不過課程一直很推薦在pycon 2014的[workshop](https://www.youtube.com/watch?v=p1iX0uxM1w8):Katharine Jarmul: Introduction to Web (and data!) Scraping with Python，約3.5個小時（因為含給聽眾的練習時間），我也強力推薦, 真的很實用，跟著做一遍就很有感覺了。雖然tutorial 也是用2.7（用3.5會有一些調整的問題）因此在本文，改成3.5的狀況提出來。
- [練習的code](https://github.com/kjam/python-web-scraping-tutorial)
- [課程投影片](https://docs.google.com/presentation/d/1uHM_esB13VuSf7O1ScGueisnrtu-6usGFD3fs4z5YCE/edit#slide=id.g1267e10d0_095)

## 2to3的差別:
1.
```python
import urllib
import urllib.request
from bs4 import BeautifulSoup
```
```python
source = urllib.request.urlopen('http://google.com')
source
```

python 3 開始沒有urllib2, 3 之分，所以要開網址的方式有改變

2.
```python
pyladies = urllib.request.urlopen('http://www.pyladies.com/')
pyladies_source = pyladies.read()
pyladies_de = pyladies_source.decode('utf8')
```
另外，python 3 讀取也要使用`decode`，才能正確讀取！

大概這兩個比較大的差別之外，還有就是print()要有雙括號才能跑得動。
python 有內建2to3的module可以協助轉換。

## 三大神器(beautiful soup, lxml, selenium)
在開始之前，可以把html/css的節點概念認知一下。如果是有學過基礎的html/javascript應該都可以很快就上手了。
firefox 和chrome內建的developer tool 是你的好朋友。

### beautiful soup
![bs4](https://dl.dropboxusercontent.com/u/22163115/Selection_014.png)
最好的方式就是好好地把[文檔](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)讀一讀。也有[中文版](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)。
```python
soup = BeautifulSoup(html_doc, 'lxml') #開始煮湯啦，'lxml'是一種parser
soup.find_all('tr') # 可以使用find_all，找到你要的
soup.find('title') # 用find的話，找到第一個就不找了
soup.head # 可以直接叫名字來存起
soup.body
```

#### install parser
在python 3下，要安裝解析器，BeautifulSoup才能好好工作
`apt-get install python lxml`
`pip install lxml`
當然還有其他的parser可用，但下一節要講lxml，這裡就用lxml為例

#### family tree
HTML的家族圖譜要搞懂，siblings, children, parents, ancestors。
這演講的這張投影片就說得很清楚:
![family tree](https://dl.dropboxusercontent.com/u/22163115/Selection_015.png)
#### css select
這演講沒有提到其實BeautifulSoup也是有css select的功能的。
但其實很簡單好用。
```python
soup.select("title")
soup.select("body a ")
soup.select(".id")
soup.select("#link")
```

### lxml
#### 安裝
如果是在ubuntu下，使用python3的話
```bash
sudo apt-get install libxml2-dev libxslt-dev python-dev
sudo apt-get install python3-lxml
```
就可完成安裝。
其他的請參考[文檔](http://lxml.de/installation.html)

#### 使用
因為這一段我也沒有好好收聽，所以只能提供官方的[tutorial](http://lxml.de/tutorial.html)。
使用上和BeautifulSoup有雷同之處，但function的名稱有所不同。
但lxml可以使用xpath 來快速定位。這是BeautifulSoup所沒有的。如果想要用xpath的話，請改用
lxml。

#### 好用的xpath
講者很快速地提過xpath的好用之處，來上這課之前，我也不太想理會xpath。不過發現真的不錯用啊。
可以在[w3cschool](http://www.w3schools.com/xsl/xpath_intro.asp)上有簡單的introduction 可以參考。
也可以看lxml的官方文件看看xpath[怎麼使用](http://lxml.de/xpathxslt.html#the-xpath-method)

### selenium
不過如果有很多互動呢？有很多js的code，不是靜態網頁呢？這個就一定要用這個selenium了！
他就是用機器模仿人類點開網站、可以輸入字串、按下enter，再回傳你要的文檔。功能強大！
因為selenium可以和很多語言都可以使用。python 的話，這裡有一個[教學](https://selenium-python.readthedocs.io/)。
```python
from selenium import webdriver
browser = webdriver.Firefox()
browser.get('http://python.org')
input = browser.find_element_by_tag_name('input')
input.send_keys('selenium', Keys.RETURN)
```
這樣就會讓firefox打開python的官網，輸入"selenium"字串，並且搜尋。
它也有很多內建的find_element(s)_by_....很多方式來去定位。

另外也有`send_keys`, 也可以要瀏覽器`wait`等等很實用的功能喔！

### 其他
csv, html, xlsx, json，許多都是python有內建的讀取文檔的方式。不再贊述。
happy scraping!!

### 附錄
[Web Scraping Indeed for Key Data Science Job Skills](https://jessesw.com/Data-Science-Skills/)
